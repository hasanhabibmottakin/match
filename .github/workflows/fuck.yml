name: Auto Update Stream API

on:
  schedule:
    - cron: "*/20 * * * *"   
  workflow_dispatch:         

permissions:
  contents: write

jobs:
  update-api:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      - name: Run scraper (inline)
        env:
          BASE_LISTING_URL: ${{ secrets.BASE_LISTING_URL }}
          BASE_STREAM_URL: ${{ secrets.BASE_STREAM_URL }}
        run: |
          python - <<'PY'
#!/usr/bin/env python3
import os, re, json, base64, requests
from bs4 import BeautifulSoup

def xor_decrypt(data_base64, key):
    try:
        decoded_bytes = base64.b64decode(data_base64)
        out = []
        klen = len(key)
        for i, b in enumerate(decoded_bytes):
            out.append(chr(b ^ ord(key[i % klen])))
        return "".join(out)
    except Exception as e:
        print(f"[ERROR] Decrypt: {e}")
        return None

def fetch_and_decrypt_stream_info(url, content_id):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
    print(f"-> Processing ID: {content_id}")
    try:
        r = requests.get(url, headers=headers, timeout=12)
        r.raise_for_status()
        html = r.text
    except Exception as e:
        print(f"[ERROR] Fetch {url}: {e}")
        return None

    key_m = re.search(r'const decryptionKey = "(.*?)";', html)
    enc_m = re.search(r'let encrypted = "(.*?)";', html)
    if not key_m or not enc_m:
        print("[WARN] missing encrypted/key")
        return None

    dec = xor_decrypt(enc_m.group(1), key_m.group(1))
    if not dec:
        return None

    mpd = re.search(r"const mpdUrl = '(.*?)';", dec)
    kid = re.search(r"const kid = '(.*?)';", dec)
    keyv = re.search(r"const key = '(.*?)';", dec)
    if not mpd or not kid or not keyv:
        print("[WARN] missing mpd/kid/key in decrypted JS")
        return None

    return {"mpdUrl": mpd.group(1), "kid": kid.group(1), "key": keyv.group(1)}

def get_channel_ids(base_listing_url):
    if not base_listing_url:
        print("[ERROR] BASE_LISTING_URL not set")
        return []
    print(f"[INFO] Fetching listing: {base_listing_url}")
    try:
        r = requests.get(base_listing_url, timeout=12, headers={'User-Agent': 'Mozilla/5.0'})
        r.raise_for_status()
        soup = BeautifulSoup(r.text, 'html.parser')
    except Exception as e:
        print(f"[FATAL] Listing fetch failed: {e}")
        return []
    ids = set()
    for link in soup.select('div.channel a[href*="play.php?id="]'):
        href = link.get('href', '')
        m = re.search(r'id=([a-fA-F0-9]+)', href)
        if m:
            ids.add(m.group(1))
    print(f"[INFO] Found {len(ids)} IDs")
    return list(ids)

def save_json(data):
    with open("api_data.json", "w") as f:
        json.dump(data, f, indent=4)
    print("[INFO] Saved api_data.json")

def main():
    base_listing = os.getenv("BASE_LISTING_URL")
    base_stream = os.getenv("BASE_STREAM_URL")
    ids = get_channel_ids(base_listing)
    if not ids:
        print("[WARN] No IDs; exiting")
        return
    try:
        with open("api_data.json", "r") as f:
            data = json.load(f)
    except Exception:
        data = {}
    for cid in ids:
        info = fetch_and_decrypt_stream_info(f"{base_stream}{cid}", cid)
        if info:
            data[cid] = info
    save_json(data)

if __name__ == "__main__":
    main()
PY

      - name: Commit and push if changed
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add api_data.json || true
          git diff --staged --quiet || (git commit -m "Auto update API data" && git push)
